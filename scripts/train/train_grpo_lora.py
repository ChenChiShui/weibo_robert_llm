import os
import torch
import warnings
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    GenerationConfig
)
from peft import LoraConfig, PeftModel
from trl import GRPOConfig, GRPOTrainer

# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# ================= 1. è·¯å¾„é…ç½® =================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(SCRIPT_DIR))

SFT_MODEL_PATH = os.path.join(PROJECT_ROOT, "model/sft_merged_model")
REWARD_MODEL_PATH = os.path.join(PROJECT_ROOT, "model/reward/checkpoint-1600")

DATA_FILE = os.path.join(PROJECT_ROOT, "processed_data/commentr_dpo_reward_data.jsonl") 
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "model/grpo")

# ================= 2. åŠ è½½ Reward Model =================
print("Loading Reward Model to GPU...")
reward_device = "cuda:0" 

def load_reward_model(model_path, device):
    is_lora = os.path.exists(os.path.join(model_path, "adapter_config.json"))
    
    if is_lora:
        print("ğŸ” æ£€æµ‹åˆ° LoRA Reward Modelï¼Œæ­£åœ¨åˆå¹¶åŠ è½½...")
        base_model_path = SFT_MODEL_PATH 
        base_model = AutoModelForSequenceClassification.from_pretrained(
            base_model_path,
            num_labels=1,
            torch_dtype=torch.bfloat16, 
            device_map=device,
            trust_remote_code=True
        )
        model = PeftModel.from_pretrained(base_model, model_path)
        model = model.merge_and_unload()
    else:
        print("ğŸ” åŠ è½½å…¨é‡ Reward Model...")
        model = AutoModelForSequenceClassification.from_pretrained(
            model_path,
            num_labels=1,
            torch_dtype=torch.bfloat16,
            device_map=device,
            trust_remote_code=True
        )
    model.eval()
    return model

torch.cuda.empty_cache()

reward_model = load_reward_model(REWARD_MODEL_PATH, reward_device)
reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_PATH, trust_remote_code=True, fix_mistral_regex=True)

if reward_tokenizer.pad_token is None:
    reward_tokenizer.pad_token = reward_tokenizer.eos_token
reward_model.config.pad_token_id = reward_tokenizer.pad_token_id

# =================================================================
# 3. å¥–åŠ±å‡½æ•° (ä¿®å¤é‡å¤æ ¼å¼åŒ–å’Œæ ‡å‡†åŒ–é—®é¢˜)
# =================================================================
def reward_func(prompts, completions, **kwargs):
    # æ‰“å°é¢„è§ˆ
    print("\n" + "="*20 + " æ¨¡å‹ç”Ÿæˆæ ·æœ¬é¢„è§ˆ " + "="*20)
    print(f"ğŸ”¹ Prompt: {prompts[0]}")
    print(f"ğŸ”¸ Completion: {completions[0]}")
    print("="*60 + "\n")

    # ä»æ ¼å¼åŒ–åçš„ prompt ä¸­æå–åŸå§‹å†…å®¹
    # æ ¼å¼: <|im_start|>user\n{original_content}<|im_end|>\n<|im_start|>assistant\n
    original_prompts = []
    for p in prompts:
        if "<|im_start|>user\n" in p and "<|im_end|>" in p:
            start = p.find("<|im_start|>user\n") + len("<|im_start|>user\n")
            end = p.find("<|im_end|>", start)
            original_content = p[start:end]
            original_prompts.append(original_content)
        else:
            original_prompts.append(p)

    inputs = []
    for p, c in zip(original_prompts, completions):
        messages = [{"role": "user", "content": p}, {"role": "assistant", "content": c}]
        try:
            text = reward_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        except:
            text = f"User: {p}\nAssistant: {c}"
        inputs.append(text)

    tokenized_inputs = reward_tokenizer(
        inputs, return_tensors="pt", padding=True, truncation=True, max_length=1024
    ).to(reward_device)

    with torch.no_grad():
        outputs = reward_model(**tokenized_inputs)
        scores = outputs.logits.squeeze(-1)

    # æ·»åŠ  4.0 çš„ offset æ¥è°ƒæ•´å¥–åŠ±åˆ†æ•°
    scores = scores + 4.0

    return scores.tolist()

# ================= 4. æ•°æ®æ ¼å¼åŒ–é€»è¾‘ (å…³é”®ä¿®æ”¹) =================
def format_data(example, tokenizer):
    # ä¿ç•™åŸå§‹ prompt ç”¨äº reward function
    # å°†åŸå§‹æ–‡æœ¬åŒ…è£…æˆ Chat æ ¼å¼ç”¨äºç”Ÿæˆï¼š
    # "<|im_start|>user\n{content}<|im_end|>\n<|im_start|>assistant\n"
    messages = [{"role": "user", "content": example["prompt"]}]
    formatted_prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    return {"prompt": formatted_prompt, "original_prompt": example["prompt"]}

# ================= 5. é…ç½® GRPO =================
training_args = GRPOConfig(
    output_dir=OUTPUT_DIR,
    run_name="commentr-reply-grpo-4b-full",
    learning_rate=1e-6,
    num_train_epochs=1,
    per_device_train_batch_size=16, 
    gradient_accumulation_steps=2, 
    beta=0.5,
    gradient_checkpointing=True,
    optim="paged_adamw_32bit",
    bf16=True,
    torch_compile=False,
    use_vllm=False, 
    num_generations=8,
    max_completion_length=512,
    logging_steps=1,
    save_steps=100,
    report_to="wandb",
)

peft_config = LoraConfig(
    task_type="CAUSAL_LM",
    r=32, lora_alpha=64, lora_dropout=0.1,
    target_modules=["q_proj", "v_proj", "down_proj", "up_proj", "gate_proj"]
)

# ================= 6. ä¸»æµç¨‹ =================
def main():
    # -------------------------------------------------------------
    # æ­¥éª¤ 1: å…ˆåŠ è½½ Tokenizer (å¿…é¡»åœ¨å¤„ç†æ•°æ®å‰)
    # -------------------------------------------------------------
    print(f"Loading Tokenizer & Checking EOS alignment...")
    tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH, trust_remote_code=True, fix_mistral_regex=True)
    
    # æ£€æŸ¥ Chat Template
    if tokenizer.chat_template is None:
        print("âš ï¸ Tokenizer æ²¡æœ‰é»˜è®¤ Chat Templateï¼Œä½¿ç”¨ ChatML æ ¼å¼")
        tokenizer.chat_template = "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"

    # EOS å¯¹é½
    correct_eos_id = tokenizer.convert_tokens_to_ids("<|im_end|>")
    if correct_eos_id == tokenizer.unk_token_id:
        correct_eos_id = tokenizer.eos_token_id
        print(f"âš ï¸ æœªæ‰¾åˆ° <|im_end|>ï¼Œå›é€€ä½¿ç”¨é»˜è®¤ EOS: {correct_eos_id}")
    else:
        print(f"ğŸ”§ æ£€æµ‹åˆ° <|im_end|> (ID: {correct_eos_id})ï¼Œå¼ºåˆ¶å°†å…¶è®¾ä¸ºç»Ÿä¸€ EOS")

    tokenizer.eos_token_id = correct_eos_id
    tokenizer.pad_token_id = correct_eos_id

    # -------------------------------------------------------------
    # æ­¥éª¤ 2: åŠ è½½æ•°æ®å¹¶åº”ç”¨æ ¼å¼åŒ–
    # -------------------------------------------------------------
    print(f"Loading Dataset from {DATA_FILE}...")
    dataset = load_dataset("json", data_files=DATA_FILE, split="train")
    
    if "input" in dataset.column_names and "prompt" not in dataset.column_names:
        dataset = dataset.rename_column("input", "prompt")
    
    print("â³ æ­£åœ¨åº”ç”¨ Chat Template æ ¼å¼åŒ–...")
    # è¿™é‡Œæ˜¯å…³é”®ï¼šå°† dataset é‡Œçš„ prompt å˜æˆå¯¹è¯æ ¼å¼
    dataset = dataset.map(lambda x: format_data(x, tokenizer), batched=False)
    
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼Œæ ·æœ¬é¢„è§ˆ: {dataset[0]['prompt'][-100:]}")

    # -------------------------------------------------------------
    # æ­¥éª¤ 3: åŠ è½½ç­–ç•¥æ¨¡å‹
    # -------------------------------------------------------------
    print(f"Loading 4B Policy Model in FULL BF16...")
    policy_model = AutoModelForCausalLM.from_pretrained(
        SFT_MODEL_PATH,
        trust_remote_code=True,
        device_map={"": 0}, 
        torch_dtype=torch.bfloat16
    )

    policy_model.config.eos_token_id = correct_eos_id
    policy_model.config.pad_token_id = correct_eos_id

    # å¤„ç† Generation Config
    if policy_model.generation_config is None:
        print("âš ï¸ æ¨¡å‹ç¼ºå°‘ generation_configï¼Œæ­£åœ¨åˆ›å»º...")
        policy_model.generation_config = GenerationConfig.from_model_config(policy_model.config)
    
    policy_model.generation_config.eos_token_id = correct_eos_id
    policy_model.generation_config.pad_token_id = correct_eos_id
    # GRPO è®­ç»ƒåˆæœŸå»ºè®®è®¾ä¸º 1.0ï¼Œé¿å…å¹²æ‰°
    policy_model.generation_config.repetition_penalty = 1.2
    print(f"âœ… GenerationConfig å·²ä¿®æ­£: EOS ID = {policy_model.generation_config.eos_token_id}, Repetition Penalty = 1.0")

    # -------------------------------------------------------------
    # æ­¥éª¤ 4: å¼€å§‹è®­ç»ƒ
    # -------------------------------------------------------------
    trainer = GRPOTrainer(
        model=policy_model, 
        reward_funcs=reward_func,
        args=training_args,
        train_dataset=dataset,
        processing_class=tokenizer,
        peft_config=peft_config,
    )
    
    print("Starting training...")
    trainer.train()
    trainer.save_model(OUTPUT_DIR)

if __name__ == "__main__":
    main()